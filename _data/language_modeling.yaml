Word_Level:
  Penn_Treebank:
  - &Yang2018d
    model:        AWD-LSTM-MoS + dynamic eval*
    authors:      Yang et al.
    year:         2018
    Validation perplexity: 48.33
    Test perplexity: 47.69
    paper:       'Breaking the Softmax Bottleneck: A High-Rank RNN Language Model'
    url:          https://arxiv.org/abs/1711.03953
    code:         []
  - &Krause2017d
    model:        AWD-LSTM + dynamic eval*
    authors:      Krause et al.
    year:         2017
    Validation perplexity: 51.6
    Test perplexity: 51.1
    paper:        Dynamic Evaluation of Neural Sequence Models
    url:          https://arxiv.org/abs/1709.07432
    code:         []
  - &Merity2017d
    model:        AWD-LSTM + continuous cache pointer*
    authors:      Merity et al.
    year:         2017
    Validation perplexity: 53.9
    Test perplexity: 52.8
    paper:        Regularizing and Optimizing LSTM Language Models
    url:          https://arxiv.org/abs/1708.02182
    code:         []
  - &Yang2018
    model:        AWD-LSTM-MoS
    authors:      Yang et al.
    year:         2018
    Validation perplexity: 56.54
    Test perplexity: 54.44
    paper: 'Breaking the Softmax Bottleneck: A High-Rank RNN Language Model'
    url:          https://arxiv.org/abs/1711.03953
    code:         []
  - &Merity2017
    model:        AWD-LSTM
    authors:      Merity et al.
    year:         2017
    Validation perplexity: 60.0
    Test perplexity: 57.3
    paper:        Regularizing and Optimizing LSTM Language Models
    url:          https://arxiv.org/abs/1708.02182
    code:         []

  WikiText_2:
  - <<:           *Yang2018d
    Validation perplexity: 42.41
    Test perplexity: 40.68
  - <<:           *Krause2017d
    Validation perplexity: 46.4
    Test perplexity: 44.3
  - <<:           *Merity2017d
    Validation perplexity: 53.8
    Test perplexity: 52.0
  - <<:           *Yang2018
    Validation perplexity: 63.88
    Test perplexity: 61.45
  - <<:           *Merity2017
    Validation perplexity: 68.6
    Test perplexity: 65.8

  WikiText_103:
  - &Rae2018
    model:        LSTM + Hebbian + Cache + MbPA
    authors:      Rae et al.
    year:         2018
    Validation perplexity: 29.0
    Test perplexity: 29.2
    paper:        Fast Parametric Learning with Activation Memorization
    url:          http://arxiv.org/abs/1803.10049
    code:         []
  - <<:           *Rae2018
    model:        LSTM + Hebbian
    Validation perplexity: 34.1
    Test perplexity: 34.3
  - <<:           *Rae2018
    model:        LSTM
    Validation perplexity: 36.0
    Test perplexity: 36.4
  - &Dauphin2016
    model:        Gated CNN
    authors:      Dauphin et al.
    year:         2016
    Validation perplexity: -
    Test perplexity: 37.2
    paper:        Language modeling with gated convolutional networks
    url:          https://arxiv.org/abs/1612.08083
    code:         []
  - &Bai2018
    model:        Temporal CNN
    authors:      Bai et al.
    year:         2018
    Validation perplexity: -
    Test perplexity: 45.2
    paper:        Convolutional sequence modeling revisited
    url:          https://openreview.net/forum?id=rk8wKk-R-.
    code:         []
  - &Graves2014
    model:        LSTM
    authors:      Graves et al.
    year:         2014
    Validation perplexity: -
    Test perplexity: 48.7
    paper:        Neural turing machines
    url:          https://arxiv.org/abs/1410.5401
    code:         []

Char_Level:
  Hutter_Prize:
  - &Al-Rfou2018_T64
    model:        T64
    authors:      Al-Rfou et al.
    year:         2018
    Bits per Character (BPC): 1.06
    Number of params (M): 235
    paper:        Character-Level Language Modeling with Deeper Self-Attention
    url:          https://arxiv.org/abs/1808.04444
    code:         []
  - <<:           *Krause2017d
    Bits per Character (BPC): 1.08
    Number of params (M): 46
  - &Al-Rfou2018_T12
    model:        T12
    authors:      Al-Rfou et al.
    year:         2018
    Bits per Character (BPC): 1.11
    Number of params (M): 44
    paper:        Character-Level Language Modeling with Deeper Self-Attention
    url:          https://arxiv.org/abs/1808.04444
    code:         []
  - &Merity2018
    model:        3 layer AWD-LSTM
    authors:      Merity et al.
    year:         2018
    Bits per Character (BPC): 1.232
    Number of params (M): 47
    paper:        An Analysis of Neural Language Modeling at Multiple Scales
    url:          https://arxiv.org/abs/1803.08240
    code:         []
  - &Mujika2017
    model:        Large FS-LSTM-4
    authors:      Mujika et al.
    year:         2017
    Bits per Character (BPC): 1.245
    Number of params (M): 47
    paper:        Fast-Slow Recurrent Neural Networks
    url:          https://arxiv.org/abs/1705.08639
    code:         []
  - &Krause2016
    model:        Large mLSTM +emb +WN +VD
    authors:      Krause et al.
    year:         2016
    Bits per Character (BPC): 1.24
    Number of params (M): 46
    paper:        Multiplicative LSTM for sequence modelling
    url:          https://arxiv.org/abs/1609.07959
    code:         []
  - <<:           *Mujika2017
    model:        FS-LSTM-4
    Bits per Character (BPC): 1.277
    Number of params (M): 27
  - &Zilly2016
    model:        Large RHN
    authors:      Zilly et al.
    year:         2016
    Bits per Character (BPC): 1.27
    Number of params (M): 46
    paper:        Recurrent Highway Networks
    url:          https://arxiv.org/abs/1607.03474
    code:         []

  Text8:
  - <<:             *Al-Rfou2018_T64
    Bits per Character (BPC): 1.13
  - <<:             *Al-Rfou2018_T12
    Bits per Character (BPC): 1.18  
  - <<:             *Krause2017d
    Bits per Character (BPC): 1.19
    Number of params (M): 45
  - <<:             *Krause2016
    Bits per Character (BPC): 1.27
    Number of params (M): 45
  - <<:             *Zilly2016
    Bits per Character (BPC): 1.27
    Number of params (M): 46
  -
    model:          LayerNorm HM-LSTM
    authors:        Chung et al.
    year:           2017
    Bits per Character (BPC): 1.29
    Number of params (M): 35
    paper:          Hierarchical Multiscale Recurrent Neural Networks
    url:            https://arxiv.org/abs/1609.01704
    code:           []
  -
    model:          BN LSTM
    authors:        Cooijmans et al.
    year:           2016
    Bits per Character (BPC): 1.36
    Number of params (M): 16
    paper:          Recurrent Batch Normalization
    url:            https://arxiv.org/abs/1603.09025
    code:           []
  - <<:             *Krause2016
    model:          Unregularised mLSTM
    Bits per Character (BPC): 1.4
    Number of params (M): 45

  Penn_Treebank:
  - <<:             *Merity2018
    Bits per Character (BPC): 1.175
    Number of params (M): 13.8
  - <<:             *Merity2018
    model:          6 layer QRNN
    Bits per Character (BPC): 1.187
    Number of params (M): 13.8
  - <<:             *Mujika2017
    model:          FS-LSTM-4
    Bits per Character (BPC): 1.19
    Number of params (M): 27.0
  - <<:             *Mujika2017
    model:          FS-LSTM-2
    Bits per Character (BPC): 1.193
    Number of params (M): 27.0
  -
    model:          NASCell
    authors:        Zoph & Le
    year:           2016
    Bits per Character (BPC): 1.214
    Number of params (M): 16.3
    paper:          Neural Architecture Search with Reinforcement Learning
    url:            https://arxiv.org/abs/1611.01578
    code:           []
  -
    model:          2-Layer Norm HyperLSTM
    authors:        Ha et al.
    year:           2016
    Bits per Character (BPC): 1.219
    Number of params (M): 14.4
    paper:          HyperNetworks
    url:            https://arxiv.org/abs/1609.09106
    code:           []
